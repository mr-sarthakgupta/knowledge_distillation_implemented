# knowledge_distillation_implemented
pyTorch implementation of the knowledge distillation to train a compact network to perform much better than it would have by "normal training".

## Colab notebook link: https://colab.research.google.com/drive/1kstBDTSvxQHcRH_KT3v0FUS88BT5SLri?usp=sharing
