# knowledge_distillation_implemented
pyTorch implementation of the knowledge distillation to train a compact network to perform much better than it would have by "normal training".
